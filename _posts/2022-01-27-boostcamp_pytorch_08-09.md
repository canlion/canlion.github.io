---
title: "pytorch_08-19. multi-gpu / hp tuning"
excerpt: "model parallel & data parallel / νλΌλ―Έν„° νλ‹μ„ μ„ν• Ray Tune"

categories:
  - boostcamp
tags:
  - [boostcamp]

toc: true
toc_sticky: true

date: 2022-01-27 10:49
last_modified_at: 2021-01-27 10:49
---

# Multi-GPU ν•™μµ

GPUκ°€ λ°μ „ν•λ©΄μ„ κ±°λ€ν• λ°μ΄ν„°, κ±°λ€ν• λ„¤νΈμ›ν¬κ°€ νΈλ λ“. 

## κ°λ… μ •λ¦¬
* single vs multi
* gpu vs node
  * node: ν• λ€μ μ‹μ¤ν…
* single node single gpu: ν• λ€μ μ‹μ¤ν…μ ν• κ° gpu
* single node multi gpu: ν• λ€μ μ‹μ¤ν…μ λ‹¤μ gpu
* multi node multi gpu: μ—¬λ¬ λ€μ μ‹μ¤ν…μ λ‹¤μ gpu

## multi gpu

### Model Parallel
λ‹¤μ¤‘ gpuμ— λ¨λΈμ„ λ‚λ„λ” λ°©μ‹. κ³Όκ±°λ¶€ν„° μ‚¬μ©λλ λ°©μ‹(ex-alexnet)μ΄μ§€λ§ λ¨λΈμ λ³‘λ©, νμ΄ν”„λΌμΈ μ–΄λ ¤μ›€ λ“±μΌλ΅ κ³ λ‚μ΄λ„.

[PyTorch:SINGLE-MACHINE MODEL PARALLEL BEST PRACTICES](https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html){: .btn .btn--info}

![model_parallel](/assets/images/post/220127/boostcamp_pytorch_08-09/model_parallel.png){: .align-center}

### Data Parallel
λ°μ΄ν„°λ¥Ό gpuμ— λ‚λ μ„ ν• λ‹Ή, κ²°κ³Όμ ν‰κ· μ„ μ·¨ν•λ‹¤.

#### μΌλ°μ μΈ Data Parallel
1. λ¨λ“  gpuμ— λ™μΌν• λ¨λΈ μ „μ†΅.
2. ν• κ°μ λ―Έλ‹ λ°°μΉ λ°μ΄ν„°λ¥Ό gpu μλ§νΌ μΌκ°μ–΄ κ° gpuμ— ν• λ‹Ή.
3. μ—°μ‚° μν–‰. 
4. κ° gpuμ λ¨λΈ outputμ„ λ©”μΈ gpuλ΅ μ „μ†΅.
5. λ©”μΈ gpuμ—μ„ κ° gpuλ΅λ¶€ν„° μ „μ†΅λ outputμ— λ€μ‘ν•λ” lossμ ν‰κ· μΌλ΅ κ·Έλλ””μ–ΈνΈ κ³„μ‚°.
6. λ©”μΈ gpuμ—μ„ κ° gpuλ΅ loss κ·Έλλ””μ–ΈνΈ μ „μ†΅.
7. κ° gpuμ—μ„ backward μν–‰, κ·Έλλ””μ–ΈνΈ κ³„μ‚°
8. κ° gpuμ—μ„ λ©”μΈ gpuλ΅ κ·Έλλ””μ–ΈνΈ μ „μ†΅
9. λ©”μΈ gpuμ—μ„ κ·Έλλ””μ–ΈνΈ ν‰κ· μΌλ΅ κ°€μ¤‘μΉ κ°±μ‹ 
10. 1~9 λ°λ³µ

![data_parallel](/assets/images/post/220127/boostcamp_pytorch_08-09/data_parallel.png){: .align-center}

λ©”μΈ gpuμ—μ„ lossμ™€ loss κ·Έλλ””μ–ΈνΈλ¥Ό κ³„μ‚°ν•κ³  κ°€μ¤‘μΉ μ—…λ°μ΄νΈλ¥Ό λ‹΄λ‹Ήν•λ―€λ΅ λ‹¤λ¥Έ gpuλ³΄λ‹¤ λ©”λ¨λ¦¬ μ‚¬μ©λ‰μ΄ λ§μ„ μ μμ(gpu μ‚¬μ© λ¶κ· ν• λ°μƒ). λ¨λ“  gpuμ—μ„ λ™μΌν• ν¬κΈ°μ λ°μ΄ν„°λ¥Ό μ²λ¦¬ν•΄μ•Όν•λ―€λ΅ λ©”μΈ gpu λ§μ¶° λ―Έλ‹ λ°°μΉ ν¬κΈ°λ¥Ό μ¤„μ—¬μ•Όν•  μλ„ μμ. + GIL

```python
parallel_model = torch.nn.DataParallel(model)
...
pred = parallel_model(X)
loss = loss_func(pred, y)
loss.mean().backward()  # loss ν‰κ· λ‚΄μ–΄ κ·Έλλ””μ–ΈνΈ κ³„μ‚°
optimizer.step()
```

#### DistributedDataParallel
μ„μ μΌλ°μ μΈ data parallelμ€ ν• ν”„λ΅μ„Έμ¤κ°€ λ©”μΈ gpuμ— λ¶™μ–΄μ„ λ¨λ“  μ‘μ—…μ„ λ‹΄λ‹Ή. distributed data parallelμ€ κ° gpuμ— ν”„λ΅μ„Έμ¤κ°€ λ¶™μ–΄μ„ μ£Όμ–΄μ§„ λ°μ΄ν„°μ— forward, backward μ§„ν–‰. λ¨λ“  gpuμ—μ„ κ·Έλλ””μ–ΈνΈκ°€ μ¤€λΉ„λλ‹¤λ©΄ allreduce μ—°μ‚°μ„ ν†µν•΄ κ·Έλλ””μ–ΈνΈ ν‰κ· λ‚΄μ–΄ κ° gpuμ—μ„ λ¨λΈ μ—…λ°μ΄νΈ μν–‰.

![dist_data_parallel](/assets/images/post/220127/boostcamp_pytorch_08-09/dist_data_parallel.png){: .align-center}

```python
sampler = torch.utils.data.distributed.DistributedSampler(data)
trainloader = torch.utils.data.DataLoader(
    data,
    batch_size=20,
    shuffle=False,
    pin_memory=True,  # gpuλ΅ λ°μ΄ν„°λ¥Ό λ³΄λ‚΄λ” κ³Όμ •μ„ κ°„μ†ν™”ν•μ—¬ λΉ λ¥΄κ²
    num_workers=(num_gpu*4)  # κ²½ν—μ μΈ λ¶€λ¶„μΈ λ“―.
)
...
```

[PyTorch:GETTING STARTED WITH DISTRIBUTED DATA PARALLEL](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html?highlight=distributed){: .btn .btn--info}

<br>

# Hyperparameter Tuning

**ν•μ΄νΌνλΌλ―Έν„°**: λ¨Έμ‹ λ¬λ‹ λ¨λΈ ν•™μµμ—μ„ μ‚¬λμ΄ μ„ νƒν•΄μ•Όν•λ” νλΌλ―Έν„° (lr, lr decay, ...)

* μ„±λ¥ ν–¥μƒμ„ μ„ν• μ‹λ„
  * λ¨λΈ λ³€κ²½
    * μ΄λ―Έ μ—¬λ¬ λ¨λΈμ΄ μ΅΄μ¬ν•κ³  μ–΄λ–¤ λ¨λΈμ΄ μΆ‹μ€μ§€λ„ λ‹¤ μ•λ‹¤.
  * π λ°μ΄ν„° κ²€μ, μ¶”κ°€
  * ν•μ΄νΌνλΌλ―Έν„° νλ‹
    * μ΄μµμ΄ κ·Έλ ‡κ² ν¬μ§€λ” μ•λ‹¤.
    * μ΅°κΈμ΄λΌλ„ μ„±λ¥ ν–¥μƒμ„ μ„ν•΄ μ •λ§ μ²μ ν•κ² λ§μ§€λ§‰κΉμ§€ λ§¤λ‹¬λ¦¬λ” λ°©λ²•
  * κΈ°νƒ€$\cdots$

## hyperparameter tuning
λ¨λΈ μ¤μ¤λ΅ ν•™μµν•μ§€ μ•λ” κ°’μ„ μ–΄λ–»κ² μ„¤μ •ν•΄μ•Όν•λ”κ°€? - ν•™μµλ¥ , λ¨λΈ κµ¬μ΅°, λ¨λΈ ν¬κΈ°, optimizer, ...

μ΄μ „μ—λ” λ„¤νΈμ›ν¬ μ„±λ¥μ„ μ¬ν„ν•λ” μΌμ— ν•μ΄νΌνλΌλ―Έν„°κ°€ κµ‰μ¥ν μ¤‘μ”ν–μ§€λ§ λ°μ΄ν„°μ μ–‘μΌλ΅ λ°€μ–΄λ¶™μ΄λ©° μ μ°¨ κ·Έ μ¤‘μ”μ„±μ€ λν•΄μ§€λ” μ¶”μ„Έ. λ°μ΄ν„°κ°€ λ§μ•„μ§€λ©΄μ„ ν•μ΄νΌνλΌλ―Έν„° νλ‹λ„ λ§λ§μΉ μ•μ€ μ‘μ—…μ΄ λμ–΄λ²„λ¦Ό. ν•μ΄νΌνλΌλ―Έν„°μ— λ”°λΌ μ„±λ¥ μ°¨μ΄κ°€ μƒκΈΈ μ μμΌλ―€λ΅ ν•™μµ μ΄κΈ°μ— μ μ ν• ν•μ΄νΌνλΌλ―Έν„°κ°’μ„ μ°ΎκΈ°μ„ν• λ…Έλ ¥μ΄ λ μλ„ μκ³ , μµν›„μ— μ•½κ°„μ μ„±λ¥μ΄λΌλ„ μ¥μ–΄μ§κΈ°μ„ν• λ„μ „μ΄ λ  μλ„ μλ‹¤.

μ΄μ „μ—λ” random search, grid searchλ¥Ό ν•¨κ» μ‚¬μ©ν•μ€μΌλ‚ μµκ·Όμ—λ” λ² μ΄μ§€μ• κΈ°λ° κΈ°λ²•λ“¤μ΄ λ€μ„Έ. (BOHB, 2018, [paper](https://ml.informatik.uni-freiburg.de/wp-content/uploads/papers/18-ICML-BOHB.pdf))

### random + grid

#### random search
* λ¬΄μ‘μ„ μ§€μ  ν•™μµ μν–‰
* μ„±λ¥μ΄ μ λ‚μ¤λ” λ€λµμ μΈ μ§€μ μ„ νƒμƒ‰

![random](/assets/images/post/220127/boostcamp_pytorch_08-09/random_search.png){: .align-center}

#### grid search
* λ³΄ν†µ μΌμ • logκ°’ κ°„κ²©μΌλ΅ λ‡κ°μ ν•μ΄νΌνλΌλ―Έν„° ν¬μΈνΈλ¥Ό λ§λ“¤κ³  ν•™μµ μν–‰
* random search ν›„ λ€λµμ μΈ μ§€μ μ—μ„ λ””ν…μΌν•κ² ν•μ΄νΌ νλΌλ―Έν„° νλ‹

![grid](/assets/images/post/220127/boostcamp_pytorch_08-09/grid_search.png){: .align-center}

## Ray

* ν•μ΄νΌνλΌλ―Έν„° νλ‹ ν΄. multi node multi processing μ§€μ›
* ML/DL λ³‘λ ¬μ²λ¦¬ μ„ν• λ¨λ“
  * λ¶„μ‚°λ³‘λ ¬ ML/DL λ¨λ“μ ν‘μ¤€
* ν•μ΄νΌνλΌλ―Έν„° μ„μΉλ¥Ό μ„ν• λ¨λ“ μ κ³µ

[PyTorch:HYPERPARAMETER TUNING WITH RAY TUNE](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html){: .btn .btn--info}
