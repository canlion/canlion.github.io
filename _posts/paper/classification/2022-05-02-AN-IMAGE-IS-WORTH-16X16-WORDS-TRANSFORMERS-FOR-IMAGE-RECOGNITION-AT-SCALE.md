---
title: "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE"
excerpt: "ViT - Vision Transformer"

categories:
  - ë…¼ë¬¸-classification
tags:
  - []

toc: true
toc_sticky: true

date: 2022-07-05T16:35:00+09:00
last_modified_at: 2022-07-05T16:35:00+09:00
---

íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” NLP ë¶„ì•¼ì—ì„œ ì‚¬ì‹¤ìƒì˜ í‘œì¤€ìœ¼ë¡œ ìë¦¬ ì¡ì•˜ìœ¼ë‚˜ CV ë¶„ì•¼ì—ì„œëŠ” ì˜ í˜ì„ ëª»ì¼ë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ë¡œ êµ¬ì„±ëœ ë¶„ë¥˜ ëª¨ë¸ì¸ **ViT(Vision Transformer)**ëŠ” CNNì— ë¹„í•´ ë¶€ì¡±í•œ ì¸ë•í‹°ë¸Œ ë°”ì´ì–´ìŠ¤ ë•Œë¬¸ì— ì¼ë°˜í™”ë˜ëŠ”ë° í° ë°ì´í„°ì…‹ì´ í•„ìš”í•˜ì§€ë§Œ í° ë°ì´í„°ì…‹ì— ëŒ€í•œ ì‚¬ì „ í•™ìŠµ í›„ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì „ì´í•™ìŠµì„ í†µí•´ ê¸°ì¡´ CNN ê³„ì—´ì˜ SOTA ëª¨ë¸ë³´ë‹¤ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤.
{:.notice--success}

<div class="notice--info" markdown="1">
**inductive bias, ì¸ë•í‹°ë¸Œ ë°”ì´ì–´ìŠ¤**
* [ìœ„í‚¤](https://en.wikipedia.org/wiki/Inductive_bias)
* ëª¨ë¸ì´ ì ‘í•œ ì  ì—†ëŠ” ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ì„ í•  ë•Œ ì´ìš©í•˜ëŠ” ê°€ì •ë“¤
  * ì ‘í•œ ì  ì—†ëŠ” ì…ë ¥ê³¼ ì¶œë ¥ì˜ ê´€ê³„ì— ëŒ€í•œ ê°€ì •
* CNN ê³„ì—´ì˜ ì¸ë•í‹°ë¸Œ ë°”ì´ì–´ìŠ¤ëŠ” locality, translation invariance
  * ì¸ì ‘í•œ í”½ì…€ë“¤ì˜ ê²°í•©ìœ¼ë¡œ íŒ¨í„´ì´ í˜•ì„±ë˜ë©°, ì´ë¯¸ì§€ì˜ ì–´ë–¤ ìœ„ì¹˜ì— íƒ€ê²Ÿ íŒ¨í„´ì´ ì¡´ì¬í•˜ë”ë¼ë„ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ íŒ¨í„´ì„ ì¡ì•„ë‚¸ë‹¤.
* ğŸ¤– ëª¨ë¸ êµ¬ì¡°, ì•Œê³ ë¦¬ì¦˜ì´ íŠ¹ì • taskì—ì„œ ì˜ ë™ì‘í•˜ê¸° ìœ„í•´ ê°–ê²Œ ëœ ì œì•½, ê°€ì •ì´ë¼ê³  ìƒê°í•˜ë©´ ì–´ë–¨ì§€? (ì˜ˆë¥¼ ë“¤ë©´ conv ë ˆì´ì–´ëŠ” ì˜¤ì§ ë°€ì§‘ëœ í”½ì…€ì—ì„œë§Œ ì •ë³´ë¥¼ ì–»ê²Œ ì„¤ê³„ë˜ì–´ CV ë¶„ì•¼ì—ì„œ ì˜ ë™ì‘í•œë‹¤?)
</div>

## íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” inductive biasê°€ ë¶€ì¡±í•´ ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•˜ë‹¤.

íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ë†’ì€ ì—°ì‚° íš¨ìœ¨ê³¼ scalability ë•ì— ê±°ëŒ€í•œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆì—ˆê³ , í° ë°ì´í„°ì…‹ì— ëŒ€í•œ ì‚¬ì „ í•™ìŠµ í›„ ì „ì´í•™ìŠµì„ í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë§ì€ taskì—ì„œ SOTAë¥¼ ë‹¬ì„±í–ˆë‹¤. CVì—ì„œë„ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì¥ì ì„ ì´ìš©í•´ë³´ë ¤ëŠ” ì‹œë„ê°€ ìˆì—ˆì§€ë§Œ í•˜ë“œì›¨ì–´ ê°€ì†ê¸°ì—ì„œ ë™ì‘ì‹œí‚¤ë ¤ë©´ ë³µì¡í•œ ì—”ì§€ë‹ˆì–´ë§ì´ í•„ìš”í•´ ì—¬ì „íˆ SOTA ëª¨ë¸ë“¤ì€ CNN ê³„ì—´ì´ì—ˆë‹¤. (ë‹¹ì‹œ SOTA: ... -> BiT -> EfficientNet: NoisyStudent)

ì €ìë“¤ì€ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì•½ê°„ ìˆ˜ì •í•˜ê³  ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜ ë‹¨ìœ„ë¡œ ìª¼ê°œ íŒ¨ì¹˜ ì„ë² ë”©ì„ ë§Œë“¤ì–´ ì´ë¯¸ì§€ë„· ë¶„ë¥˜ ëª¨ë¸ì„ í•™ìŠµì‹œì¼°ë‹¤**(ViT: Vision Transformer)**. ì„±ëŠ¥ì€ ë¹„ìŠ·í•œ í¬ê¸°ì˜ ResNetë³´ë‹¤ ë‹¤ì†Œ ë‚®ì•˜ëŠ”ë° ì €ìë“¤ì€ ì´ ê²°ê³¼ë¥¼ ì¸ë•í‹°ë¸Œ ë°”ì´ì–´ìŠ¤ì˜ ë¶€ì¡±ìœ¼ë¡œ ëª¨ë¸ì´ ì¼ë°˜í™”ë˜ê¸° ìœ„í•œ ë°ì´í„°ê°€ ì¶©ë¶„ì¹˜ ì•Šë‹¤ëŠ” í•´ì„ì„ ë‚´ë¦°ë‹¤.

ì €ìë“¤ì˜ í•´ì„ëŒ€ë¡œ ê±°ëŒ€í•œ ë°ì´í„°ì…‹ì„ ì´ìš©í•˜ëŠ” ê²½ìš° ìƒí™©ì´ ë‹¬ë¼ì§„ë‹¤. ì´ë¯¸ì§€ë„·-21k, JFT-300M ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ì „ í•™ìŠµì„ ì‹œí‚¨ í›„ ë‹¤ë¥¸ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì— ì „ì´í•™ìŠµ ì‹œí‚¤ë©´ ê¸°ì¡´ CNN ê³„ì—´ì˜ SOTA ëª¨ë¸ë“¤ë³´ë‹¤ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤.

## ViT: Vision Transformer

![ViT architecture](../../../assets/images/post/paper/classification/2022-05-02-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE/ViT-architecture.jpg){:.align-center}

### íŒ¨ì¹˜ ì„ë² ë”©

í‘œì¤€ íŠ¸ëœìŠ¤í¬ë¨¸ì™€ ë§ˆì°¬ê°€ì§€ë¡œ ViTë„ ì‹œí€€ìŠ¤ë¥¼ ë‹¤ë£¬ë‹¤. ì´ë¯¸ì§€ë¥¼ ì¡°ê°ì¡°ê° íŒ¨ì¹˜ë¡œ ì˜ë¼ì„œ flatten í›„ linear projectionì„ í†µí•´ ì„ë² ë”©ì„ ë§Œë“ ë‹¤. BERTì˜ [class] í† í°ê³¼ ìœ ì‚¬í•˜ê²Œ í•™ìŠµ ê°€ëŠ¥í•œ ì„ë² ë”©ì„ íŒ¨ì¹˜ ì„ë² ë”© ì•ì— ë§ë¶™ì¸ë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ìµœì¢… ì¶œë ¥ì—ì„œ ì´ ì„ë² ë”©ì€ ì–´í…ì…˜ì„ í†µí•´ ì´ë¯¸ì§€ ì „ì²´ì˜ ì •ë³´ë¥¼ ë‹´ì€ ì´ë¯¸ì§€ ë¦¬í”„ë ˆì  í…Œì´ì…˜ $y$ ì—­í• ì„ í•œë‹¤. íŒ¨ì¹˜ ì„ë² ë”©ì— í¬ì§€ì…˜ ì¸ì½”ë”©ì„ ë”í•´ íŒ¨ì¹˜ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ ë‹´ëŠ”ë° í‘œì¤€ íŠ¸ëœìŠ¤í¬ë¨¸ì™€ëŠ” ë‹¤ë¥´ê²Œ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•œë‹¤.

![ì´ë¯¸ì§€ ë¶„í• ](../../../assets/images/post/paper/classification/2022-05-02-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE/split_img.jpg){:.align-center}

![patch embedding](../../../assets/images/post/paper/classification/2022-05-02-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE/patch-embedding.jpg){:.align-center}

* $\mathbf{z}_0$: ì…ë ¥ ì‹œí€€ìŠ¤, íŒ¨ì¹˜ ì„ë² ë”©
* $\mathbf{x}_{class}$: class í† í° (ì´ë¯¸ì§€ ë¦¬í”„ë ˆì  í…Œì´ì…˜ì„ ë‹´ëŠ” ì—­í• )
* $\mathbf{x}^i_p$: flattenëœ ië²ˆì§¸ íŒ¨ì¹˜
* $\mathbf{x}^i_p\mathbf{ E}$: íŒ¨ì¹˜ ì‹œí€€ìŠ¤ì˜ linear projection, $\mathbf{E}\in\mathbb{R}^{P^2\cdot C \times D}$
  * $P$: íŒ¨ì¹˜ ì‚¬ì´ì¦ˆ
  * $D$: linear projection ì°¨ì›
  * $C$: ì´ë¯¸ì§€ ì±„ë„ ìˆ˜
* $\mathbf{E}_{pos} \in \mathbb{R}^{(N+1)\times D}$: í¬ì§€ì…˜ ì¸ì½”ë”©
  * $N$: ì „ì²´ íŒ¨ì¹˜ì˜ ìˆ˜
  * $N+1$: class í† í°ì´ ì¶”ê°€ë˜ì–´ +1

```python
...
from einops.layers.torch import Rearrange
from einops import repeat


class PatchEmbedding(nn.Module):
    def __init__(self, img_size=28, ch=1, patch_size=7, n_patch=4, d=16, drop_p=.0):
        super().__init__()
        self.img_size = img_size
        self.patch_size = patch_size
        self.n_patch = n_patch
        self.d = d
        
        self.lin_proj = nn.Linear(patch_size*patch_size*ch, d)
        self.drop = nn.Dropout(drop_p)
        
        # ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜ ë‹¨ìœ„ë¡œ ìª¼ê°œëŠ” ë ˆì´ì–´
        self.rearrange = Rearrange('b c (h_n h_p) (w_n w_p) -> b (h_n w_n) (c h_p w_p)', w_n = n_patch, h_n = n_patch)
        
        # í´ë˜ìŠ¤ í† í°ê³¼ í¬ì§€ì…˜ ì¸ì½”ë”© - í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë¥¼ ì´ìš©
        self.class_token = torch.nn.Parameter(torch.randn(1, 1, d))
        self.pos_enc = torch.nn.Parameter(torch.randn(1, n_patch*n_patch+1, d))
        
    def forward(self, x):
        # x: (B, C, img_size, img_size)
        # x_reshape: (B, n_patch, patch_size*patch_size*C)
        # embedding: (B, n_patch+1, d)
        
        B = x.size()[0]
        
        x = self.rearrange(x)  # íŒ¨ì¹˜ ë‹¨ìœ„ë¡œ ë¶„í• 
        x = self.lin_proj(x)  # projection
        x = self.drop(x)
        
        # í´ë˜ìŠ¤ í† í°ì„ ë§ë¶™ì´ê³  í¬ì§€ì…˜ ì¸ì½”ë”©ì„ ë”í•œë‹¤.
        cls_tok_repeat = repeat(self.class_token, 'b n d -> (batch b) n d', batch=B)
        x = torch.cat([cls_tok_repeat, x], dim=1)
        x = self.pos_enc + x
        x = self.drop(x)
        
        return x
```

### íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”

í‘œì¤€ íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œ ì•½ê°„ ìˆ˜ì •ë˜ì—ˆë‹¤. ë ˆì´ì–´ ì •ê·œí™”ê°€ MSA(Multiheaded Self-Attention) ë ˆì´ì–´ì™€ MLP ë ˆì´ì–´ ì „ìœ¼ë¡œ ë‚´ë ¤ì™”ë‹¤. MSA ë ˆì´ì–´ëŠ” í‘œì¤€ íŠ¸ëœìŠ¤í¬ë¨¸ì™€ ë™ì¼í•˜ë©° MLP ë ˆì´ì–´ëŠ” 2ê°œì˜ linear ë ˆì´ì–´ ì‚¬ì´ì— GELUë¥¼ activationìœ¼ë¡œ ì‚¬ìš©í•œë‹¤. í‘œì¤€ íŠ¸ëœìŠ¤í¬ë¨¸ì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì¸ì½”ë”ë¥¼ ì¤‘ì²©í•˜ì—¬ í™œìš©í•œë‹¤.

![ì¸ì½”ë” ì—°ì‚°](../../../assets/images/post/paper/classification/2022-05-02-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE/enc_compute.jpg){:.align-center}

* $\mathbf{z}_l$: $l$ë²ˆì§¸ ì¸ì½”ë” ì¶œë ¥

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d=16, n_head=2, drop_p=.0):
        super().__init__()
        self.lin_Q = nn.Linear(d, d)
        self.lin_K = nn.Linear(d, d)
        self.lin_V = nn.Linear(d, d)
        self.lin_O = nn.Linear(d, d)
        self.drop = nn.Dropout(drop_p)
        
        self.n_head = n_head
        self.scale_factor = torch.sqrt(torch.tensor(d / n_head))
        
    def forward(self, x):
        B, N, d = x.size()
        
        # ì„ë² ë”© ì‹œí€€ìŠ¤ë¥¼ Query, Key, Valueë¡œ ë³€í™˜
        Q = self.lin_Q(x)
        K = self.lin_K(x)
        V = self.lin_V(x)
        
        # multihead attentionì„ ìˆ˜í–‰í•˜ë„ë¡ Q, K, Vë¥¼ n_headê°œë¡œ ë¶„í• 
        Q = Q.reshape(B, N, self.n_head, -1).transpose(1, 2)  # (B, n_head, N, d_k)
        K = K.reshape(B, N, self.n_head, -1).transpose(1, 2)
        V = V.reshape(B, N, self.n_head, -1).transpose(1, 2)
        
        # attention
        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale_factor  # (B, n_head, N, N)
        attention_weights = torch.softmax(attention_scores, dim=-1)
        attention_values = torch.matmul(attention_weights, V)  # (B, n_head, N, d_v)
        
        # ì—¬ëŸ¬ headë“¤ì˜ valueë¥¼ ë‹¤ì‹œ ëª¨ì•„ í•˜ë‚˜ì˜ ì‹œí€€ìŠ¤ë¡œ projection
        values_concat = attention_values.permute(0, 2, 1, 3).reshape(B, N, -1)
        output = self.lin_O(values_concat)
        output = self.drop(output)
        
        return output
    

class MLP(nn.Module):
    def __init__(self, d=16, multiple=4, drop_p=.0):
        super().__init__()
        self.lin_0 = nn.Linear(d, d * multiple)
        self.gelu = nn.GELU()
        self.lin_1 = nn.Linear(d * multiple, d)
        
        self.drop = nn.Dropout(drop_p)
        
    def forward(self, x):
        x = self.lin_0(x)
        x = self.gelu(x)  # GELU!
        x = self.drop(x)
        x = self.lin_1(x)
        x = self.drop(x)
        return x
    

class EncoderBlock(nn.Module):
    def __init__(self, d=16, n_head=2, multiple=4, drop_p=.0):
        super().__init__()
        
        self.ln_mha = nn.LayerNorm(d)
        self.mha = MultiHeadAttention(d, n_head, drop_p)
        self.ln_mlp = nn.LayerNorm(d)
        self.mlp = MLP(d, multiple, drop_p)
        
    def forward(self, x):
        x_ln = self.ln_mha(x)  # ë ˆì´ì–´ ì •ê·œí™”ê°€ ê° ì„œë¸Œë ˆì´ì–´ ì•ì— ìœ„ì¹˜
        x_mha = self.mha(x_ln)
        x = x_mha + x  # residual connection
        
        x_ln = self.ln_mlp(x)
        x_mlp = self.mlp(x_ln)
        x = x_mlp + x
        return x
```

### ViT

ë§ˆì§€ë§‰ íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”ì˜ ì¶œë ¥ì—ì„œ ì²«ë²ˆì§¸ ì„ë² ë”©($\mathbf{y}=\mathbf{z}_L^0$)ì„ 1ì¸µ MLPì— ì…ë ¥í•´ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•œë‹¤. ê° ë ˆì´ì–´ì— ì¶”ê°€ëœ dropoutê³¼ classification head ì´í›„ì˜ tanhëŠ” ë…¼ë¬¸ì˜ appendixì—ì„œ ì°¾ì•„ë³¼ ìˆ˜ ìˆë‹¤.

conv ì—°ì‚°ì„ í†µí•´ ë„¤íŠ¸ì›Œí¬ ì „ì²´ì— locality, translation invarianceê°€ ë…¹ì•„ìˆëŠ” CNNì— ë¹„í•´ ViTëŠ” ì¸ë•í‹°ë¸Œ ë°”ì´ì–´ìŠ¤ê°€ ì ë‹¤. ì²˜ìŒ ì‘ê²Œì‘ê²Œ íŒ¨ì¹˜ë¥¼ ìª¼ê°œ ì„ë² ë”©ì„ ë§Œë“œëŠ” ë¶€ë¶„ê³¼ MLP ì—°ì‚°ì—ì„œë§Œ locality, translation invariance íŠ¹ì„±ì´ ìˆê³  ê·¸ ì™¸ì—ëŠ” ëª¨ë‘ globalí•˜ë‹¤.

ì´ë¯¸ì§€ì— ë°”ë¡œ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì—°ê²°í•˜ì§€ ì•Šê³  CNNì˜ í”¼ì³ë§µì— íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì—°ê²°í•œ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ë„ ê°€ëŠ¥í•˜ë‹¤.

![ì´ë¯¸ì§€ ë¦¬í”„ë ˆì  í…Œì´ì…˜](../../../assets/images/post/paper/classification/2022-05-02-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE/img-representation.jpg){:.align-center}

```python
class ViT(nn.Module):
    def __init__(self, num_classes=10, n_enc=2, img_size=28, img_ch=1, patch_size=7, n_patch=4, d=16, n_head=2, multiple=4, drop_p=.0):
        super().__init__()
        self.emb_layer = PatchEmbedding(img_size, img_ch, patch_size, n_patch, d, drop_p)
        self.enc_list = nn.ModuleList([EncoderBlock(d, n_head, multiple, drop_p) for _ in range(n_enc)])
        self.enc_out_ln = nn.LayerNorm(d)
        self.cls_head = nn.Linear(d, num_classes)
        
    def forward(self, x):
        x = self.emb_layer(x)
        
        # ì¸ì½”ë” ì¤‘ì²© ì—°ì‚°
        for enc in self.enc_list:
            x = enc(x)
        
        # ë ˆì´ì–´ ì •ê·œí™”
        x = self.enc_out_ln(x[:, 0])  # z^0_L = y
        x = self.cls_head(x)
        x = torch.tanh(x)
        
        return x
```

<div class="notice--info" markdown="1">
MNIST ë°ì´í„°ë¥¼ CrossEntropyLoss, Adam, 15 Epochs í•™ìŠµì‹œí‚¤ë©´ validation ì •í™•ë„ ì•½ 96%
```python
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
â”œâ”€PatchEmbedding: 1-1                    --
|    â””â”€Linear: 2-1                       800
|    â””â”€Dropout: 2-2                      --
|    â””â”€Rearrange: 2-3                    --
â”œâ”€ModuleList: 1-2                        --
|    â””â”€EncoderBlock: 2-4                 --
|    |    â””â”€LayerNorm: 3-1               32
|    |    â””â”€MultiHeadAttention: 3-2      1,088
|    |    â””â”€LayerNorm: 3-3               32
|    |    â””â”€MLP: 3-4                     2,128
|    â””â”€EncoderBlock: 2-5                 --
|    |    â””â”€LayerNorm: 3-5               32
|    |    â””â”€MultiHeadAttention: 3-6      1,088
|    |    â””â”€LayerNorm: 3-7               32
|    |    â””â”€MLP: 3-8                     2,128
â”œâ”€LayerNorm: 1-3                         32
â”œâ”€Linear: 1-4                            170
=================================================================
Total params: 7,562
Trainable params: 7,562
Non-trainable params: 0
=================================================================
```
</div>

## ViT vs. CNN

ViTì™€ SOTA CNNì˜ ì„±ëŠ¥ì„ ë¹„êµí•´ë³¸ë‹¤.

### ì‹¤í—˜ ëª¨ë¸
* ViT: 'ViT-ëª¨ë¸í¬ê¸°/íŒ¨ì¹˜í¬ê¸°'ë¡œ í‘œê¸°. íŒ¨ì¹˜í¬ê¸°ê°€ ì‘ì„ìˆ˜ë¡ ì‹œí€€ìŠ¤ê°€ ê¸¸ì–´ì ¸ ì—°ì‚°ì´ ì»¤ì§„ë‹¤. ì¸ì½”ë”ì˜ ìˆ˜, MLP ì‚¬ì´ì¦ˆ, $D$ë¥¼ í†µí•´ ì‚¬ì´ì¦ˆ ì¡°ì ˆ
  * Huge: ViT-H/14
  * Large: ViT-L/16, Vit-L/14
  * Base: ViT-B/16, Vit-B/14
* BiT: ResNet ê¸°ë°˜ì˜ SOTA (20ë…„ ì´ˆ)
* EfficientNet-NoisyStudent: ìµœì‹  SOTA (20ë…„ ì´ˆ)
  * ì´ë¯¸ì§€ë„·-21k, JFT-300Mì„ semi-supervised learning

### ì‹¤í—˜
ì´ë¯¸ì§€ë„·-21k, JFT-300Mìœ¼ë¡œ ViT, BiTë¥¼ ì‚¬ì „í•™ìŠµ ì‹œí‚¨ í›„ ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì— ì „ì´í•™ìŠµì‹œì¼œ ì„±ëŠ¥ì„ ë¹„êµ. (EfficientNetì€ NoisyStuendent ë°©ì‹ìœ¼ë¡œ í•™ìŠµ)

**ì‚¬ì „ í•™ìŠµì— í•„ìš”í•œ ë¦¬ì†ŒìŠ¤ì™€ ì„±ëŠ¥ ë¹„êµ**

![sotaì™€ ë¹„êµ](../../../assets/images/post/paper/classification/2022-05-02-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE/comparison-sota.jpg){:.align-center}

ì•½ê°„ ì‘ì€ ViT-L ëª¨ë¸ë¡œë„ BiT ëª¨ë¸ë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ëƒˆê³  ê°€ì¥ í° ViT-HëŠ” EfficientNetë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ëƒˆë‹¤. í•˜ë‹¨ì˜ TPUv3-core-daysëŠ” ì‚¬ì „í•™ìŠµì— ì‚¬ìš©ëœ TPU ì½”ì–´ ìˆ˜ì™€ ì‚¬ì „í•™ìŠµ ê¸°ê°„(ì¼ ë‹¨ìœ„)ì˜ ê³±ìœ¼ë¡œ ì‘ì„ìˆ˜ë¡ ì‚¬ì „í•™ìŠµì— í•„ìš”í•œ ì—°ì‚° ë¦¬ì†ŒìŠ¤ê°€ ì ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. ViTê°€ í•„ìš”ë¡œ í•˜ëŠ” ì—°ì‚° ë¦¬ì†ŒìŠ¤ê°€ ë‹¤ë¥¸ ëª¨ë¸ë“¤ì— ë¹„í•´ í›¨ì”¬ ì‘ìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.

**ë°ì´í„° í•„ìš”ëŸ‰**

ViTê°€ ì–¼ë§ˆë‚˜ ë§ì€ ì‚¬ì „í•™ìŠµ ë°ì´í„°ê°€ í•„ìš”í•œì§€ ì•Œì•„ë³´ëŠ” ì‹¤í—˜. ì´ë¯¸ì§€ë„·, ì´ë¯¸ì§€ë„·-21k, JFT-300M, JFT-300Mì˜ ì„œë¸Œì…‹ìœ¼ë¡œ ViT, BiTë¥¼ í•™ìŠµì‹œì¼œ 1) ì´ë¯¸ì§€ë„·ì— íŒŒì¸íŠœë‹ í›„ ì •í™•ë„ ë¹„êµ 2) few-shot evaluation ë¹„êµ

![ë°ì´í„° í•„ìš”ëŸ‰ ì‹¤í—˜](../../../assets/images/post/paper/classification/2022-05-02-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE/data-requirements-exp.jpg){:.align-center}

ì¢Œì¸¡ ê·¸ë˜í”„) ë°ì´í„°ì…‹ í¬ê¸°ë³„ë¡œ ì´ë¯¸ì§€ë„· íŒŒì¸íŠœë‹ ì •í™•ë„ë¥¼ ë¹„êµí•œë‹¤. ì¤‘ê°„ ì‚¬ì´ì¦ˆì˜ ì´ë¯¸ì§€ë„· ë°ì´í„°ì…‹ì—ì„œëŠ” ViTê°€ BiTë³´ë‹¤ ë‚®ì€ ì„±ëŠ¥ì„ ê¸°ë¡í•˜ê³  ë˜ ViT ë¼ì§€ ëª¨ë¸ì´ ë² ì´ìŠ¤ ëª¨ë¸ë³´ë‹¤ ë‚®ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤. í•˜ì§€ë§Œ ì ì°¨ ë°ì´í„°ì…‹ì´ ì»¤ì§ˆìˆ˜ë¡ ViTê°€ BiTë³´ë‹¤, ë¼ì§€ëª¨ë¸ì´ ë² ì´ìŠ¤ëª¨ë¸ë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¸ë‹¤. 

ìš°ì¸¡ ê·¸ë˜í”„) JFT-300M ë°ì´í„°ì…‹ì˜ ì„œë¸Œì…‹ì„ ì ì°¨ í‚¤ì›Œê°€ë©° ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ. ë§ˆì°¬ê°€ì§€ë¡œ ì ì€ ë°ì´í„°ì—ì„œëŠ” BiTê°€ ìš°ì„¸í•˜ë‚˜ ë°ì´í„°ê°€ ë§ì•„ì§ˆìˆ˜ë¡ ViTê°€ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ë©° ì„±ëŠ¥ ì¦ê°€ì˜ ë‘”í™”ë„ ë” ëŠ¦ê²Œ ë‚˜íƒ€ë‚œë‹¤. 

ì´ ë‘ ì‹¤í—˜ì—ì„œ CNNì€ ì‘ì€ ë°ì´í„°ì…‹ì—ì„œ ì„±ëŠ¥ì´ ì¢‹ê³ , ViTëŠ” í° ë°ì´í„°ì…‹ì—ì„œ ì„±ëŠ¥ì´ ì¢‹ìŒì„ ì•Œ ìˆ˜ ìˆì—ˆë‹¤. ì´ë¥¼ í†µí•´ ì¸ë•í‹°ë¸Œ ë°”ì´ì–´ìŠ¤ëŠ” ì‘ì€ ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆë„ë¡ í•˜ì§€ë§Œ ë°ì´í„°ê°€ ì¶©ë¶„íˆ ë§ì€ ê²½ìš°ì—ëŠ” ì¸ë•í‹°ë¸Œ ë°”ì´ì–´ìŠ¤ ì—†ì´ íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆìœ¼ë©° ë” ìœ ìµí•¨ì„ ë³´ì—¬ì¤€ë‹¤.

**ì‚¬ì „í•™ìŠµì— í•„ìš”í•œ ì—°ì‚°ëŸ‰ê³¼ í¼í¬ë¨¼ìŠ¤ ë¹„êµ - scaling study**

![resource vs performance](../../../assets/images/post/paper/classification/2022-05-02-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE/pre-train-resource-vs-performance.jpg){:.align-center}

ì‚¬ì „í•™ìŠµì— í•„ìš”í•œ ì—°ì‚°ëŸ‰ê³¼ ê·¸ì— ë”°ë¥¸ í¼í¬ë¨¼ìŠ¤ë¥¼ ë¹„êµí•˜ëŠ” ì‹¤í—˜ìœ¼ë¡œ ì„¸ ì¢…ë¥˜ì˜ ëª¨ë¸ì„ í¬ê¸°ì— ë”°ë¼ JFT-300M ë°ì´í„°ì…‹ì„ 7 epochs, 14epochs ì‚¬ì „ í•™ìŠµ í›„ íŒŒì¸íŠœë‹ ì‹œì¼œ ì´ë¯¸ì§€ë„· ì •í™•ë„ë¥¼ ë¹„êµí•œë‹¤. 

ìœ„ ê·¸ë˜í”„ì˜ ê°€ë¡œì¶•ì€ ì‚¬ì „ í•™ìŠµì— í•„ìš”í•œ ì—°ì‚°ëŸ‰, ì„¸ë¡œì¶•ì€ ì´ë¯¸ì§€ë„· ì •í™•ë„. ê°™ì€ ì‚¬ì „í•™ìŠµ ì—°ì‚°ëŸ‰ì—ì„œ ViTê°€ BiTë³´ë‹¤ ë†’ì€ ì„±ëŠ¥ì„ ë‚´ë©° ì—°ì‚°ëŸ‰ê³¼ í¼í¬ë¨¼ìŠ¤ íŠ¸ë ˆì´ë“œ ì˜¤í”„ì—ì„œ ìœ ë¦¬í•¨ì„ ë³´ì—¬ì¤€ë‹¤. í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ì€ ViTë³´ë‹¤ ë” ë‚˜ì€ ëª¨ìŠµì„ ë³´ì´ë‹¤ê°€ ì ì°¨ ë¹„ìŠ·í•´ì§€ê±°ë‚˜ ë‚˜ë¹ ì§„ë‹¤.

ê²Œë‹¤ê°€ ì •í™•ë„ê°€ ê³„ì† ìƒìŠ¹í•˜ë©° ì‚¬ì „í•™ìŠµì„ ë” ê¸¸ê²Œ ìˆ˜í–‰í•˜ë©´ ë” ë†’ì€ ì„±ëŠ¥ë„ ë‹¬ì„±í•  ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.

ğŸ¤–: í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ì´ ì ì°¨ ViTì™€ì˜ ì„±ëŠ¥ ê²©ì°¨ê°€ ì¤„ì–´ë“œëŠ” ê²ƒì€ ì¸ë•í‹°ë¸Œ ë°”ì´ì–´ìŠ¤ê°€ ë¹ ë¥´ê²Œ íŒ¨í„´ì„ ë½‘ì•„ë‚´ì§€ë§Œ í•™ìŠµì´ ê³„ì† ë ìˆ˜ë¡ ì ì°¨ íŒ¨í„´ í•™ìŠµì— ë°©í•´ê°€ ë˜ëŠ”ê±´ê°€?
{: .notice--danger}

![resource vs performance table](../../../assets/images/post/paper/classification/2022-05-02-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE/pre-train-resource-vs-performance-table.jpg){:.align-center}
ìœ„ ì‹¤í—˜ì„ ì •ë¦¬í•œ í…Œì´ë¸”. ë˜‘ê°™ì´ 7epochsì„ í•™ìŠµí•˜ê³  ë¹„ìŠ·í•œ ì—°ì‚°ëŸ‰ì´ ì¸¡ì •ëœ ViTì™€ BiTë¥¼ ë¹„êµí–ˆì„ë•Œ ì„±ëŠ¥ì°¨ì´ê°€ í¬ë‹¤. ì¶©ë¶„íˆ ë°ì´í„°ê°€ ë§ì€ ê²½ìš°ì— ë¹„ìŠ·í•œ í¬ê¸°ì˜ ëª¨ë¸ë¡œ ë˜‘ê°™ì´ ë°ì´í„°ë¥¼ ë³´ì—¬ì¤€ë‹¤ë©´ ViTê°€ BiTë³´ë‹¤ í›¨ì”¬ ì˜ í•™ìŠµí•œë‹¤... ì‹ ê¸°í•˜ë„¤
{: .notice--info}


## ViT ë‚´ë¶€

![ViT ë‚´ë¶€](../../../assets/images/post/paper/classification/2022-05-02-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE/inspecting-vit.jpg){:.align-center}


### figure 7 left: ì„ë² ë”© ë ˆì´ì–´ì˜ í•„í„°ëŠ” CNNì˜ ì €ì¸µ ë ˆì´ì–´ì²˜ëŸ¼ ê¸°ì´ˆì ì¸ íŠ¹ì§•ì„ ì¡ì•„ë‚¸ë‹¤.

ì„ë² ë”© ë ˆì´ì–´ëŠ” flattenëœ íŒ¨ì¹˜ë¥¼ ì„ë² ë”©ìœ¼ë¡œ projectioní•˜ëŠ”ë° í•„í„°ë¥¼ ì‹œê°í™”í•˜ë©´ CNNì˜ ì €ì¸µ ë ˆì´ì–´ ì»¤ë„ ì‹œê°í™” ê²°ê³¼ì™€ ë¹„ìŠ·í•˜ê²Œ ì„ , ì ë“¤ì„ ë³¼ ìˆ˜ ìˆë‹¤.

![alexnet kernel](../../../assets/images/post/paper/classification/2022-05-02-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE/alexnet-kernel-visualization.jpg){:.align-center}

ì¶œì²˜: **ImageNet Classification with Deep Convolutional
Neural Networks**
{:.text-center}

### figure 7 center: ì¸ì ‘í•œ ìœ„ì¹˜ì˜ í¬ì§€ì…˜ ì„ë² ë”©ë¼ë¦¬, ê°™ì€ í–‰ê³¼ ì—´ì— ì†í•˜ëŠ” ì„ë² ë”©ë¼ë¦¬ ìœ ì‚¬ë„ê°€ ë†’ê²Œ í•™ìŠµëœë‹¤.

íŒ¨ì¹˜ì˜ 2D ìœ„ì¹˜ ì •ë³´ë¥¼ ì‚¬ëŒì´ ë„£ì–´ì£¼ì§€ ì•Šì•„ë„ ì•Œì•„ì„œ ê·¼ì ‘í•œ í¬ì§€ì…˜ ì„ë² ë”©ë¼ë¦¬, ê°™ì€ í–‰ê³¼ ì—´ì— ì†í•œ í¬ì§€ì…˜ ì„ë² ë”©ë¼ë¦¬ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ê°€ ë†’ì•„ì§€ë„ë¡ í•™ìŠµëœë‹¤.

í¬ì§€ì…˜ ì„ë² ë”©ì˜ ìœ ë¬´ì— ë”°ë¥¸ ì„±ëŠ¥ì°¨ì´ëŠ” í™•ì—°í•˜ì§€ë§Œ 1D í¬ì§€ì…˜ ì„ë² ë”©ì´ë‚˜ 2D í¬ì§€ì…˜ ì„ë² ë”© ë“± ì„ë² ë”©ì˜ í˜•ì‹ì— ë”°ë¥¸ ì„±ëŠ¥ì°¨ì´ëŠ” ì—†ë‹¤. ì €ìë“¤ì€ í¬ì§€ì…˜ ì„ë² ë”©ì´ í”½ì…€ ë‹¨ìœ„ì˜ ìœ„ì¹˜ê°€ ì•„ë‹Œ íŒ¨ì¹˜ ë‹¨ìœ„ì˜ ìœ„ì¹˜ë¥¼ í‘œí˜„í•˜ë¯€ë¡œ spatial representationì˜ í•™ìŠµ ë‚œì´ë„ê°€ ë‚®ì•„ ì–´ë–¤ ì„ë² ë”© í˜•ì‹ì„ ì‚¬ìš©í•˜ë˜ ë¬´ê´€í•œ ê²ƒìœ¼ë¡œ ì˜ˆìƒí–ˆë‹¤.

```python
import timm


# timmì—ì„œ í•™ìŠµëœ ViTë¥¼ í•˜ë‚˜ ì¡ì•„ì™€ í¬ì§€ì…˜ ì„ë² ë”©ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì‹œê°í™”
m = timm.create_model('vit_base_patch32_224', pretrained=True)
m.eval()
pos_enc = m.pos_embed

n = 7
pos_enc_grid = einops.rearrange(pos_enc[0, 1:], '(h w) d -> h w d', h=n)

plt.figure(figsize=(10, 10))
for i in range(n**2):
    r = i // n
    c = i % n
    
    with torch.no_grad():
        result = cos(pos_enc_grid, pos_enc_grid[r, c])
    
    plt.subplot(n, n, i+1)
    plt.imshow(result.cpu().numpy())
    plt.axis('off')
plt.show()
```

![cos similarity](../../../assets/images/post/paper/classification/2022-05-02-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE/pos-emb-cos-sim.png){:.align-center}

### figure 7 right: ëŠ” ì˜ ëª¨ë¥´ê² ëŠ”ë°... ë” ê³µë¶€í•˜ê³  ì¶”ê°€í•˜ê¸°ë¡œ...


"attention distance"ê°€ ë­”ì§€ ëª¨ë¥´ê² ë‹¤. ê³µë¶€ ì¤‘...
{:.notice--warning}

## ê²°ë¡ 

ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ì œì™¸í•˜ë©´ ì´ë¯¸ì§€ì— íŠ¹í™”ëœ ì–´ë–¤ ì¸ë•í‹°ë¸Œ ë°”ì´ì–´ìŠ¤ë„ ì—†ì´ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì´ë¯¸ì§€ ë¶„ë¥˜ ë¬¸ì œì— ì ìš©í–ˆë‹¤. ì‚¬ì „í•™ìŠµ ë°ì´í„°ì…‹ì´ ì¶©ë¶„íˆ í¬ë‹¤ë©´ CNN ê³„ì—´ SOTA ëª¨ë¸ë“¤ë³´ë‹¤ í›¨ì”¬ ì ì€ ì‚¬ì „í•™ìŠµ ë¦¬ì†ŒìŠ¤ë¡œë„ ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ë¹„ìŠ·í•˜ê±°ë‚˜ ë” ë†’ì€ ì„±ëŠ¥ì„ ë‚¸ë‹¤. ë¶„ë¥˜ ë¬¸ì œì—ì„œì˜ ì„±ê³µì— ë”°ë¼ ë””í…ì…˜, ì„¸ê·¸ë©˜í…Œì´ì…˜ë¿ë§Œ ì•„ë‹ˆë¼ self-supervised learning ë“± ì—¬ëŸ¬ ë¶„ì•¼ì—ì„œì˜ ì—°êµ¬ê°€ ê¸°ëŒ€ëœë‹¤.